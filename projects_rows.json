[
  {
    "id": 10,
    "title": "SeeForMe",
    "description": "AI-powered visual assistance platform that provides real-time environmental navigation, image description, and color blindness tools for visually impaired users using YOLO v11 and LLM integration.",
    "long_description": "SeeForMe is an open-source, AI-driven platform designed to be the eyes for people who are blind or visually impaired. Built for a healthcare hackathon with a focus on inclusivity and accessibility, this comprehensive solution addresses the daily challenges faced by over 2.2 billion people worldwide who live with visual impairments. The platform combines cutting-edge computer vision, natural language processing, and real-time communication technologies to provide context-aware assistance that goes beyond simple object identification to deliver actionable guidance for independent navigation and daily tasks.",
    "image": "https://pyeklitbktoezqmbwbvx.supabase.co/storage/v1/object/public/portfolio/projects/1750616143252.png",
    "category": "hackathon",
    "position": "2nd Place",
    "tags": "[\"React\",\"Python\",\"Flask\",\"YOLO\",\"Computer Vision\",\"AI\",\"Accessibility\",\"Healthcare\",\"Socket.IO\",\"OpenCV\",\"TailwindCSS\",\"Web Speech API\",\"GROQ\",\"Gemini API\",\"Real-time\",\"Mobile\"]",
    "github": "https://github.com/datican-run-undergraduates-competition/SeeForMe",
    "demo": "https://drive.google.com/file/d/1BUHk98j47bvK1wOdKaRzWyFT57OTFzGK/view?usp=drive_link",
    "features": "[\"Real-time environmental navigation with custom-trained YOLO v11 object detection\",\"Instant image description using Gemini API with voice output\",\"Color blindness simulation tools for multiple vision deficiencies (Protanopia, Deuteranopia, Tritanopia, Achromatopsia)\",\"Voice-controlled navigation and commands via Web Speech API\",\"Hands-free keyboard navigation shortcuts for accessibility\",\"Live camera streaming with Socket.IO for real-time guidance\",\"Context-aware navigation instructions powered by GROQ LLM\",\"Cross-platform compatibility (web/mobile) with responsive design\",\"Open-source codebase focused on accessibility and inclusivity\"]",
    "challenges": "[\"AI Model Integration: Successfully combined YOLO v11 object detection with LLM reasoning to provide contextual, actionable navigation instructions rather than simple object identification\",\"Real-time Performance: Implemented efficient Socket.IO streaming to handle live camera feeds while maintaining low latency for critical navigation guidance\",\"Cross-platform Accessibility: Addressed platform-specific limitations (iOS voice restrictions) while ensuring consistent user experience across devices\",\"Custom YOLO Training: Developed specialized object detection model trained on environmental obstacles and navigation-relevant objects for improved accuracy\",\"Voice Interface Design: Created intuitive voice command system with clear audio feedback, essential for users who cannot see visual confirmations\",\"Accessibility-First UI: Designed interface following WCAG guidelines with high contrast, screen reader compatibility, and keyboard-only navigation support\"]",
    "overview": "An AI-powered visual assistance platform that provides real-time environmental navigation, image description, and color blindness tools for visually impaired users using YOLO v11 and LLM integration.",
    "system_design": "## Architecture Overview\r\n\r\nSeeForMe employs a sophisticated multi-tier architecture that seamlessly integrates computer vision, natural language processing, and real-time communication to deliver comprehensive visual assistance.\r\n\r\n### Frontend Architecture (React + Vite)\r\n\r\nThe frontend is built using React with Vite for optimal performance and development experience, featuring:\r\n\r\n- **Component Structure**:\r\n  - Main Dashboard with feature selection\r\n  - Camera Interface for real-time navigation\r\n  - Image Upload/Capture components\r\n  - Color Blindness Simulation tools\r\n  - Voice Command Interface\r\n  - Accessibility Navigation components\r\n\r\n- **Key Technologies**:\r\n  - **TailwindCSS**: Utility-first styling with accessibility-focused design\r\n  - **Socket.IO Client**: Real-time bidirectional communication\r\n  - **Web Speech API**: Voice recognition and text-to-speech functionality\r\n  - **Gemini API Integration**: Direct frontend image description processing\r\n  - **Responsive Design**: Mobile-first approach with cross-device compatibility\r\n\r\n- **Accessibility Features**:\r\n  - ARIA labels and semantic HTML structure\r\n  - High contrast color schemes\r\n  - Keyboard navigation support\r\n  - Screen reader compatibility\r\n  - Voice command integration\r\n\r\n### Backend Architecture (Python + Flask)\r\n\r\nThe backend serves as the AI processing engine, handling complex computer vision and natural language tasks:\r\n\r\n- **Core Framework**:\r\n  - **Flask**: Lightweight web framework for API endpoints\r\n  - **Flask-SocketIO**: Real-time WebSocket communication\r\n  - **OpenCV**: Computer vision and image processing\r\n  - **Ultralytics YOLO v11**: Custom-trained object detection model\r\n\r\n- **AI Pipeline**:\r\n  - **YOLO v11 Object Detection**: Custom-trained model for environmental object recognition\r\n  - **GROQ LLM Integration**: Context-aware instruction generation\r\n  - **Real-time Processing**: Efficient frame processing for live camera feeds\r\n  - **Multi-threaded Processing**: Concurrent handling of multiple user sessions\r\n\r\n### Real-Time Communication Flow\r\n\r\n```\r\nFrontend Camera → Socket.IO → Backend Processing → YOLO Detection → LLM Context → Voice Instruction → Frontend Audio Output\r\n```\r\n\r\n### AI Model Integration\r\n\r\n**YOLO v11 Custom Training**:\r\n- Trained on environmental obstacles and navigation-relevant objects\r\n- Optimized for real-time inference on mobile devices\r\n- Custom classes for accessibility-specific scenarios\r\n\r\n**LLM Context Processing**:\r\n- GROQ API integration for natural language instruction generation\r\n- Context-aware responses based on detected objects and user location\r\n- Concise, actionable guidance optimized for audio delivery\r\n\r\n**Gemini API Integration**:\r\n- Frontend-direct image description processing\r\n- Multi-modal understanding for complex scene analysis\r\n- Real-time image-to-text conversion with accessibility focus\r\n\r\n### Data Flow Architecture\r\n\r\n1. **Real-time Navigation**:\r\n   - Camera stream → Frontend capture → Socket.IO transmission\r\n   - Backend YOLO processing → Object detection → LLM context generation\r\n   - Instruction delivery → Text-to-speech → User audio feedback\r\n\r\n2. **Image Description**:\r\n   - Image capture/upload → Gemini API processing\r\n   - Description generation → Voice output → User feedback\r\n\r\n3. **Color Blindness Simulation**:\r\n   - Image processing → Frontend color transformation\r\n   - Multiple deficiency type simulation → Visual comparison display\r\n\r\n### Security & Performance\r\n\r\n- **Secure Communication**: HTTPS/WSS protocols for all data transmission\r\n- **Efficient Processing**: Optimized YOLO inference for mobile devices\r\n- **Error Handling**: Comprehensive fallback mechanisms for AI service failures\r\n- **Privacy-First**: No permanent storage of user images or personal data\r\n\r\n### Deployment Architecture\r\n\r\n- **Frontend**: Static deployment optimized for CDN delivery\r\n- **Backend**: Containerized Flask application with auto-scaling capabilities\r\n- **AI Models**: Optimized model serving with GPU acceleration support\r\n- **Real-time Infrastructure**: WebSocket connection management with session handling",
    "database_design": null,
    "system_design_image": null,
    "database_design_image": null,
    "additional_details": "The SeeForMe platform represents a significant advancement in assistive technology, combining multiple AI disciplines to create a comprehensive solution for visual accessibility. The project demonstrates innovative approaches to real-time computer vision processing, context-aware AI instruction generation, and accessible user interface design. The successful integration of YOLO v11 object detection with LLM reasoning creates a unique system that provides actionable guidance rather than simple object identification, addressing a critical gap in existing assistive technologies. The platform's open-source nature and focus on accessibility standards ensure broad compatibility and community-driven improvements, making it a valuable contribution to the assistive technology ecosystem.",
    "project_date": "2024-07-13",
    "team_size": 3,
    "team_members": "[\"Taiwo Ayomide\",\"Omidire Ifeoluwa\"]",
    "duration": "1 month",
    "is_published": true,
    "created_at": "2025-01-30 14:42:57.08369+00",
    "updated_at": "2025-01-30 14:42:57.08369+00"
  }
]
